# 医疗设备模块爬虫详细文档

## 📋 目录
- [美国（US）爬虫](#美国us爬虫)
- [欧盟（EU）爬虫](#欧盟eu爬虫)
- [中国（CN）爬虫](#中国cn爬虫)

---

## 🇺🇸 美国（US）爬虫

### 表1：美国FDA爬虫列表

| 爬虫名称 | 目标实体 | 数据来源 | 主要方法 | 输入参数 | 输出结果 |
|---------|---------|---------|---------|---------|---------|
| **US_510K** | Device510K | FDA 510K数据库 | `crawlAndSaveDevice510K()` | `searchTerm`: 搜索词<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: "保存成功: X 条新记录, 跳过重复: Y 条" |
| **US_510K** | Device510K | FDA 510K数据库 | `crawlAndSaveByDeviceName()` | `deviceName`: 设备名称<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: 保存结果描述 |
| **US_510K** | Device510K | FDA 510K数据库 | `crawlAndSaveByApplicant()` | `applicant`: 申请人名称<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: 保存结果描述 |
| **US_510K** | Device510K | FDA 510K数据库 | `crawlAndSaveByTradeName()` | `tradeName`: 商品名称<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: 保存结果描述 |
| **US_510K** | Device510K | FDA 510K数据库 | `crawlAndSaveWithKeywords()` | `keywords`: 关键词列表<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: 保存结果汇总 |
| **US_recall_api** | DeviceRecallRecord | FDA设备召回数据库 | `crawlAndSaveDeviceRecall()` | `searchTerm`: 搜索词<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: "保存成功: X 条新记录, 跳过重复: Y 条" |
| **US_recall_api** | DeviceRecallRecord | FDA设备召回数据库 | `crawlAndSaveWithKeywords()` | `keywords`: 关键词列表<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: 批量爬取结果 |
| **US_event_api** | DeviceEventReport | FDA不良事件报告 | `crawlMAUDEDataWithParams()` | `brandName`: 品牌名称<br>`manufacturer`: 制造商<br>`modelNumber`: 型号<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期<br>`maxPages`: 最大页数 | Map: {totalSaved, totalSkipped, totalPages} |
| **US_event_api** | DeviceEventReport | FDA不良事件报告 | `crawlFDADataWithKeywords()` | `keywords`: 关键词列表<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期<br>`maxPages`: 最大页数 | Map: {totalSaved, totalSkipped, results} |
| **US_event** | DeviceEventReport | FDA不良事件报告 | `crawlMAUDEData()` | `deviceName`: 设备名称<br>`manufacturer`: 制造商<br>`brandName`: 品牌<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | Map: 爬取结果统计 |
| **US_registration** | DeviceRegistrationRecord | FDA设备注册数据库 | `crawlAndSaveDeviceRegistration()` | `searchTerm`: 搜索词<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: "保存成功: X 条新记录" |
| **US_registration** | DeviceRegistrationRecord | FDA设备注册数据库 | `crawlAndSaveByProprietaryName()` | `proprietaryName`: 专有名称<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: 保存结果描述 |
| **US_registration** | DeviceRegistrationRecord | FDA设备注册数据库 | `crawlAndSaveByManufacturerName()` | `manufacturerName`: 制造商名称<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: 保存结果描述 |
| **US_registration** | DeviceRegistrationRecord | FDA设备注册数据库 | `crawlAndSaveByDeviceName()` | `deviceName`: 设备名称<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: 保存结果描述 |
| **US_registration** | DeviceRegistrationRecord | FDA设备注册数据库 | `crawlAndSaveWithKeywords()` | `keywords`: 关键词列表<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: 批量爬取结果 |
| **US_Guidance** | GuidanceDocument | FDA指导文档 | `crawl()` | 无参数 | void (直接保存到数据库) |
| **US_Guidance** | GuidanceDocument | FDA指导文档 | `crawlWithLimit()` | `maxRecords`: 最大记录数 | void (直接保存到数据库) |
| **US_CustomsCase** | CustomsCase | CBP海关裁定 | `crawlAndSaveCustomsCases()` | `searchTerm`: 搜索词（HS编码或关键词）<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`startDate`: 开始日期 | List<CustomsCase>: 保存的记录列表 |
| **US_CustomsCase** | CustomsCase | CBP海关裁定 | `crawlByHsCode()` | `hsCode`: HS编码<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`startDate`: 开始日期 | List<CustomsCase>: 保存的记录列表 |
| **US_CustomsCase** | CustomsCase | CBP海关裁定 | `crawlByKeyword()` | `keyword`: 搜索关键词<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`startDate`: 开始日期 | List<CustomsCase>: 保存的记录列表 |
| **US_CustomsCase** | CustomsCase | CBP海关裁定 | `crawlWithKeywords()` | `keywords`: 关键词列表<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | String: "总共保存 X 条记录" |

### 详细说明：美国爬虫

#### 1. US_510K - FDA 510K设备申请爬虫

**数据源：** `https://api.fda.gov/device/510k.json`

**核心方法：**

```java
// 方法1：通用爬取
public String crawlAndSaveDevice510K(
    String searchTerm,     // 搜索词，格式如 "device_name:pacemaker"
    int maxRecords,        // 最大记录数，-1表示全部
    int batchSize,         // 批次大小，建议50
    String dateFrom,       // 开始日期 "yyyyMMdd"
    String dateTo          // 结束日期 "yyyyMMdd"
)

// 方法2：按设备名称爬取
public String crawlAndSaveByDeviceName(
    String deviceName,     // 设备名称，如"pacemaker"
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)

// 方法3：按申请人爬取
public String crawlAndSaveByApplicant(
    String applicant,      // 申请人名称
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)

// 方法4：批量关键词爬取（推荐）
public String crawlAndSaveWithKeywords(
    List<String> keywords, // 关键词列表 ["pacemaker", "cardiac", "implant"]
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)
```

**输入示例：**
```java
// 示例1：按设备名称爬取
crawlAndSaveByDeviceName("pacemaker", 100, 50, "20240101", "20241231")

// 示例2：批量关键词爬取
List<String> keywords = Arrays.asList("skin analyzer", "imaging system", "monitor");
crawlAndSaveWithKeywords(keywords, 200, 50, "20240101", "20241231")
```

**输出示例：**
```
"保存成功: 45 条新记录, 跳过重复: 12 条"
```

---

#### 2. US_recall_api - FDA召回记录爬虫

**数据源：** `https://api.fda.gov/device/recall.json`

**核心方法：**

```java
// 方法1：通用爬取
public String crawlAndSaveDeviceRecall(
    String searchTerm,     // 搜索词
    int maxRecords,        // 最大记录数
    int batchSize,         // 批次大小
    String dateFrom,       // 开始日期
    String dateTo          // 结束日期
)

// 方法2：批量关键词爬取
public String crawlAndSaveWithKeywords(
    List<String> keywords, // 关键词列表
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)
```

**输入示例：**
```java
crawlAndSaveDeviceRecall("product_description:ventilator", 50, 20, "20240101", "20241231")
```

**输出示例：**
```
"保存成功: 23 条新记录, 跳过重复: 5 条"
```

---

#### 3. US_event_api - FDA不良事件报告爬虫

**数据源：** FDA MAUDE数据库

**核心方法：**

```java
// 方法1：参数化爬取
public Map<String, Object> crawlMAUDEDataWithParams(
    String brandName,      // 品牌名称
    String manufacturer,   // 制造商
    String modelNumber,    // 型号
    String dateFrom,       // 开始日期
    String dateTo,         // 结束日期
    Integer maxPages       // 最大页数
)

// 方法2：关键词批量爬取
public Map<String, Object> crawlFDADataWithKeywords(
    List<String> keywords, // 关键词列表
    String dateFrom,
    String dateTo,
    Integer maxPages
)
```

**输入示例：**
```java
crawlMAUDEDataWithParams("Medtronic", "", "", "01/01/2024", "12/31/2024", 10)
```

**输出示例：**
```json
{
  "totalSaved": 120,
  "totalSkipped": 15,
  "totalPages": 10,
  "results": [...]
}
```

---

#### 4. US_registration - FDA设备注册爬虫

**数据源：** `https://api.fda.gov/device/registrationlisting.json`

**核心方法：**

```java
// 方法1：通用爬取
public String crawlAndSaveDeviceRegistration(
    String searchTerm,
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)

// 方法2：按专有名称爬取
public String crawlAndSaveByProprietaryName(
    String proprietaryName, // 专有名称
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)

// 方法3：按制造商名称爬取
public String crawlAndSaveByManufacturerName(
    String manufacturerName,
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)

// 方法4：批量关键词爬取
public String crawlAndSaveWithKeywords(
    List<String> keywords,
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)
```

**输入示例：**
```java
crawlAndSaveByManufacturerName("Abbott Laboratories", 100, 50, "", "")
```

**输出示例：**
```
"保存成功: 67 条新记录"
```

---

#### 5. US_Guidance - FDA指导文档爬虫

**数据源：** FDA官网指导文档页面（网页爬取）

**核心方法：**

```java
// 方法1：默认爬取所有
public void crawl()

// 方法2：限制数量爬取
public void crawlWithLimit(
    int maxRecords         // 最大记录数，-1表示全部
)
```

**输入示例：**
```java
crawlWithLimit(50)  // 爬取最新的50条指导文档
```

**输出结果：** 直接保存到数据库，控制台输出爬取进度

---

#### 6. US_CustomsCase - CBP海关裁定爬虫

**数据源：** US Customs and Border Protection官网

**核心方法：**

```java
// 方法1：通用爬取
public List<CustomsCase> crawlAndSaveCustomsCases(
    String searchTerm,     // 搜索词（HS编码或关键词）
    int maxRecords,
    int batchSize,
    LocalDate startDate    // 开始日期
)

// 方法2：按HS编码爬取
public List<CustomsCase> crawlByHsCode(
    String hsCode,         // HS编码，如"9018"
    int maxRecords,
    int batchSize,
    LocalDate startDate
)

// 方法3：按关键词爬取
public List<CustomsCase> crawlByKeyword(
    String keyword,        // 关键词
    int maxRecords,
    int batchSize,
    LocalDate startDate
)

// 方法4：批量关键词爬取
public String crawlWithKeywords(
    List<String> keywords,
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)
```

**输入示例：**
```java
crawlByHsCode("9018", 100, 20, LocalDate.of(2024, 1, 1))
```

**输出示例：**
```java
List<CustomsCase> // 返回保存的CustomsCase对象列表
// 或 String: "总共保存 45 条记录"
```

---

### 美国爬虫API端点映射

| API端点 | 对应爬虫 | HTTP方法 |
|---------|---------|---------|
| `/us-crawler/execute/us510k` | US_510K | POST |
| `/us-crawler/execute/recall` | US_recall_api | POST |
| `/us-crawler/execute/event` | US_event_api | POST |
| `/us-crawler/execute/registration` | US_registration | POST |
| `/us-crawler/execute/guidance` | US_Guidance | POST |
| `/us-crawler/execute/customs-case` | US_CustomsCase | POST |

---

## 🇪🇺 欧盟（EU）爬虫

### 表2：欧盟爬虫列表

| 爬虫名称 | 目标实体 | 数据来源 | 主要方法 | 输入参数 | 输出结果 |
|---------|---------|---------|---------|---------|---------|
| **Eu_registration** | DeviceRegistrationRecord | EUDAMED数据库 | `crawlAndSaveToDatabase()` | `searchTerm`: 搜索词<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | int: 保存的记录数量 |
| **Eu_registration** | DeviceRegistrationRecord | EUDAMED数据库 | `crawlAndSaveWithKeywords()` | `keywords`: 关键词列表<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | Map: {totalSaved, totalSkipped, successCount, failureCount} |
| **Eu_recall** | DeviceRecallRecord | EUDAMED召回数据库 | `crawlAndSaveToDatabase()` | `searchTerm`: 搜索关键词<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | int: 保存的记录数量 |
| **Eu_recall** | DeviceRecallRecord | EUDAMED召回数据库 | `crawlAndSaveWithKeywords()` | `keywords`: 关键词列表<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小<br>`dateFrom`: 开始日期<br>`dateTo`: 结束日期 | Map: {totalSaved, successCount, failureCount, keywordResults} |
| **Eu_guidance** | GuidanceDocument | 欧盟医疗设备新闻 | `crawlAndSaveToDatabase()` | `maxPages`: 最大页数 | int: 保存的记录数量 |
| **Eu_customcase** | CustomsCase | TARIC数据库 | `crawlAndSaveToDatabase()` | `taricCode`: TARIC编码<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小 | int: 保存的记录数量 |
| **Eu_customcase** | CustomsCase | TARIC数据库 | `crawlAndSaveWithTaricCodes()` | `taricCodes`: TARIC编码列表<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小 | Map: {totalSaved, successCount, failureCount} |
| **Eu_customcase** | CustomsCase | TARIC数据库 | `crawlAndSaveWithKeywords()` | `keywords`: 关键词列表<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小 | String: "总共保存 X 条记录" |
| **Eu_BTI** | CustomsCase | 欧盟BTI裁定 | `crawlBTIRulings()` | `productKeyword`: 产品关键词<br>`maxPages`: 最大页数 | List: BTI裁定列表 |
| **Eu_Manufacturers** | - | 欧盟制造商数据库 | `crawlManufacturers()` | `searchTerm`: 搜索词<br>`maxPages`: 最大页数 | List: 制造商信息列表 |

### 详细说明：欧盟爬虫

#### 1. Eu_registration - EUDAMED设备注册爬虫

**数据源：** `https://ec.europa.eu/tools/eudamed/api/devices/udiDiData`

**核心方法：**

```java
// 方法1：通用爬取
public int crawlAndSaveToDatabase(
    String searchTerm,     // 搜索词
    int maxRecords,        // 最大记录数，-1表示全部
    int batchSize,         // 批次大小，建议50
    String dateFrom,       // 开始日期
    String dateTo          // 结束日期
)

// 方法2：批量关键词爬取（推荐）
public Map<String, Object> crawlAndSaveWithKeywords(
    List<String> keywords, // 关键词列表
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)
```

**输入示例：**
```java
List<String> keywords = Arrays.asList("surgical", "cardiac", "implant");
Map<String, Object> result = crawlAndSaveWithKeywords(keywords, 100, 50, "", "");
```

**输出示例：**
```json
{
  "totalSaved": 156,
  "totalSkipped": 23,
  "successCount": 3,
  "failureCount": 0,
  "keywordResults": {
    "surgical": 67,
    "cardiac": 45,
    "implant": 44
  }
}
```

---

#### 2. Eu_recall - 欧盟设备召回爬虫

**数据源：** EUDAMED召回数据库 + Safety Gate

**核心方法：**

```java
// 方法1：基础爬取
public int crawlAndSaveToDatabase(
    String searchTerm,     // 搜索关键词
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)

// 方法2：批量关键词爬取
public Map<String, Object> crawlAndSaveWithKeywords(
    List<String> keywords,
    int maxRecords,
    int batchSize,
    String dateFrom,
    String dateTo
)
```

**输入示例：**
```java
crawlAndSaveToDatabase("surgical mask", 50, 20, "2024-01-01", "2024-12-31")
```

**输出示例：**
```
23  // 返回保存的记录数
```

---

#### 3. Eu_guidance - 欧盟医疗设备新闻爬虫

**数据源：** `https://health.ec.europa.eu/medical-devices-topics-interest/latest-updates_en`

**核心方法：**

```java
// 爬取指定页数的新闻
public int crawlAndSaveToDatabase(
    int maxPages          // 最大页数
)
```

**输入示例：**
```java
crawlAndSaveToDatabase(10)  // 爬取10页新闻
```

**输出示例：**
```
45  // 返回保存的新闻数量
```

---

#### 4. Eu_customcase - TARIC关税措施爬虫

**数据源：** 欧盟TARIC数据库

**核心方法：**

```java
// 方法1：按TARIC编码爬取
public int crawlAndSaveToDatabase(
    String taricCode,      // TARIC编码，如"9018"
    int maxRecords,
    int batchSize
)

// 方法2：批量TARIC编码爬取
public Map<String, Object> crawlAndSaveWithTaricCodes(
    List<String> taricCodes, // TARIC编码列表
    int maxRecords,
    int batchSize
)

// 方法3：关键词爬取
public String crawlAndSaveWithKeywords(
    List<String> keywords,
    int maxRecords,
    int batchSize
)
```

**输入示例：**
```java
List<String> taricCodes = Arrays.asList("9018", "9019", "9021");
crawlAndSaveWithTaricCodes(taricCodes, 100, 30)
```

**输出示例：**
```json
{
  "totalSaved": 234,
  "successCount": 3,
  "failureCount": 0,
  "taricResults": {
    "9018": 120,
    "9019": 67,
    "9021": 47
  }
}
```

---

#### 5. Eu_BTI - 欧盟BTI裁定爬虫

**数据源：** 欧盟BTI裁定数据库

**核心方法：**

```java
public List<Map<String, String>> crawlBTIRulings(
    String productKeyword, // 产品关键词
    int maxPages          // 最大页数
)
```

**输入示例：**
```java
crawlBTIRulings("medical device", 5)
```

**输出示例：**
```java
List<Map<String, String>>  // BTI裁定信息列表
```

---

#### 6. Eu_Manufacturers - 欧盟制造商信息爬虫

**数据源：** EUDAMED制造商数据库

**核心方法：**

```java
public List<Map<String, String>> crawlManufacturers(
    String searchTerm,     // 搜索词
    int maxPages          // 最大页数
)
```

**输入示例：**
```java
crawlManufacturers("Germany", 10)
```

**输出示例：**
```java
List<Map<String, String>>  // 制造商信息列表
```

---

## 🇨🇳 中国（CN）爬虫

### 表3：中国爬虫列表

| 爬虫名称 | 目标实体 | 数据来源 | 主要方法 | 输入参数 | 输出结果 |
|---------|---------|---------|---------|---------|---------|
| **cn_registration** | DeviceRegistrationRecord | NMPA医疗器械数据库 | `crawlAndSave()` | `searchTerm`: 搜索词<br>`maxRecords`: 最大记录数<br>`batchSize`: 批次大小 | int: 保存的记录数 |
| **ChinaCustoms** | CustomsCase | 中国海关数据库 | `crawlCustomsData()` | `hsCode`: HS编码<br>`maxRecords`: 最大记录数 | List<CustomsCase>: 海关案例列表 |

---

## 📊 通用参数说明

### 输入参数详解

| 参数名 | 类型 | 说明 | 示例值 |
|--------|------|------|--------|
| **searchTerm** | String | 通用搜索词，支持FDA API查询语法 | `"device_name:pacemaker"`<br>`"applicant:Medtronic"` |
| **keywords** | List<String> | 关键词列表，批量搜索 | `["pacemaker", "cardiac", "implant"]` |
| **maxRecords** | int | 最大记录数，-1表示不限制 | `100` (爬取100条)<br>`-1` (爬取所有) |
| **batchSize** | int | 批次保存大小 | `50` (每50条保存一次) |
| **maxPages** | int | 最大页数 | `10` (爬取10页) |
| **dateFrom** | String | 开始日期 | `"20240101"` (FDA格式)<br>`"2024-01-01"` (通用格式)<br>`"01/01/2024"` (美式格式) |
| **dateTo** | String | 结束日期 | `"20241231"` |
| **deviceName** | String | 设备名称 | `"pacemaker"` |
| **applicant** | String | 申请人/制造商 | `"Medtronic Inc"` |
| **tradeName** | String | 商品名称/品牌 | `"HeartGuard Pro"` |
| **hsCode** | String | HS编码 | `"9018"` |
| **taricCode** | String | TARIC编码 | `"9018"` |
| **startDate** | LocalDate | 开始日期（Java对象） | `LocalDate.of(2024, 1, 1)` |

### 输出结果详解

#### 字符串格式输出
```
格式1: "保存成功: 45 条新记录, 跳过重复: 12 条"
格式2: "总共保存 67 条记录"
格式3: "保存成功: 23 条新记录"
```

#### Map格式输出
```json
{
  "totalSaved": 120,        // 保存的记录数
  "totalSkipped": 15,       // 跳过的重复记录数
  "successCount": 3,        // 成功的关键词数
  "failureCount": 0,        // 失败的关键词数
  "totalPages": 10,         // 总页数
  "keywordResults": {       // 每个关键词的结果
    "keyword1": 45,
    "keyword2": 38
  }
}
```

#### List格式输出
```java
List<CustomsCase>           // 实体对象列表
List<Map<String, String>>  // 原始数据Map列表
```

---

## 🔧 使用建议

### 1. 批量爬取模式（推荐）

```java
// 使用关键词列表批量爬取，效率最高
List<String> keywords = Arrays.asList("pacemaker", "defibrillator", "stent");

// 美国510K
us510k.crawlAndSaveWithKeywords(keywords, 500, 50, "20240101", "20241231");

// 欧盟注册
euRegistration.crawlAndSaveWithKeywords(keywords, 500, 50, "", "");

// 召回数据
usRecall.crawlAndSaveWithKeywords(keywords, 200, 30, "20240101", "20241231");
```

### 2. 单次爬取模式

```java
// 爬取特定设备名称
us510k.crawlAndSaveByDeviceName("cardiac pacemaker", 100, 50, "", "");

// 爬取特定制造商
usRegistration.crawlAndSaveByManufacturerName("Abbott Laboratories", 100, 50, "", "");

// 爬取特定HS编码
usCustoms.crawlByHsCode("9018", 50, 20, LocalDate.of(2024, 1, 1));
```

### 3. 不限制数量爬取（慎用）

```java
// maxRecords设为-1，爬取所有数据
us510k.crawlAndSaveDevice510K("device_name:medical", -1, 50, "", "");
// ⚠️ 注意：可能耗时很长，建议在夜间执行
```

---

## 📈 性能参数建议

| 爬虫类型 | 建议batchSize | 建议maxRecords | 预估耗时 |
|---------|--------------|----------------|----------|
| **510K** | 50 | 100 | 2-5分钟 |
| **召回** | 30 | 50 | 3-8分钟 |
| **事件报告** | 20 | 100 | 5-10分钟 |
| **注册** | 50 | 100 | 3-6分钟 |
| **指导文档** | - | 50 | 5-15分钟 |
| **海关案例** | 20 | 50 | 10-20分钟 |

---

## 🎯 实际调用示例

### 通过USCrawlerController调用

```bash
# 1. 爬取510K数据
curl -X POST http://localhost:8080/us-crawler/execute/us510k \
  -H "Content-Type: application/json" \
  -d '{
    "deviceName": "pacemaker",
    "dateFrom": "20240101",
    "dateTo": "20241231",
    "maxPages": 5
  }'

# 2. 使用关键词批量爬取
curl -X POST http://localhost:8080/us-crawler/execute/us510k \
  -H "Content-Type: application/json" \
  -d '{
    "inputKeywords": ["pacemaker", "defibrillator", "stent"],
    "dateFrom": "20240101",
    "dateTo": "20241231",
    "maxPages": 10
  }'

# 3. 爬取召回数据
curl -X POST http://localhost:8080/us-crawler/execute/recall \
  -H "Content-Type: application/json" \
  -d '{
    "productDescription": "ventilator",
    "dateFrom": "20240101",
    "dateTo": "20241231",
    "maxRecords": 50
  }'

# 4. 爬取海关案例
curl -X POST http://localhost:8080/us-crawler/execute/customs-case \
  -H "Content-Type: application/json" \
  -d '{
    "hsCode": "9018",
    "maxRecords": 100,
    "batchSize": 20,
    "startDate": "01/01/2024"
  }'
```

---

## 📝 爬取结果存储

所有爬虫的数据都会保存到对应的数据库表中：

| 爬虫 | 目标表 | 主键字段 |
|------|--------|----------|
| US_510K, D_510K | `t_device_510k` | `k_number` |
| US_recall_api, D_recall, Eu_recall | `t_device_recall` | `id` (自增) |
| US_event_api, US_event | `t_device_event` | `report_number` |
| US_registration, Eu_registration, cn_registration | `t_device_registration` | `id` (自增) |
| US_Guidance, Eu_guidance | `t_guidance_document` | `id` (自增) |
| US_CustomsCase, Eu_customcase, ChinaCustoms | `t_customs_case` | `id` (自增) |

---

## 🚨 注意事项

### 1. API限制
- FDA API有速率限制：240次/分钟（使用API Key）
- 建议在爬取批次之间添加延迟（1-2秒）
- 避免并发请求过多

### 2. 去重机制
- 所有爬虫都有自动去重功能
- 基于唯一标识符（k_number、report_number等）
- 重复数据会被跳过，不会重复保存

### 3. 错误处理
- 所有爬虫都有重试机制（通常3次）
- 失败的批次会记录日志
- 部分失败不影响整体进度

### 4. 日期格式
- FDA API: `yyyyMMdd` (如 `20240101`)
- 通用格式: `yyyy-MM-dd` (如 `2024-01-01`)
- 美式格式: `MM/dd/yyyy` (如 `01/01/2024`)

### 5. 关键词输入
- 支持单个关键词：`"pacemaker"`
- 支持多个关键词：`["pacemaker", "cardiac"]`
- 支持短语：`"cardiac pacemaker system"`

---

## 🔍 爬虫选择指南

### 按数据类型选择

| 需求 | 推荐爬虫 | 说明 |
|------|---------|------|
| 设备审批信息 | US_510K, Eu_registration | 获取设备批准/注册记录 |
| 召回通知 | US_recall_api, Eu_recall | 获取召回和安全警报 |
| 不良事件 | US_event_api, US_event | 获取设备事故报告 |
| 监管指南 | US_Guidance, Eu_guidance | 获取监管政策和新闻 |
| 海关归类 | US_CustomsCase, Eu_customcase | 获取HS编码裁定 |

### 按国家选择

| 目标市场 | 使用爬虫 |
|---------|---------|
| 美国市场 | US_510K, US_recall_api, US_event_api, US_registration, US_Guidance, US_CustomsCase |
| 欧盟市场 | Eu_registration, Eu_recall, Eu_guidance, Eu_customcase, Eu_BTI |
| 中国市场 | cn_registration, ChinaCustoms |

---

## 📖 完整示例

### 示例1：爬取心脏起搏器相关数据

```java
// 1. 爬取510K申请
List<String> keywords = Arrays.asList("pacemaker", "cardiac", "implant");
us510k.crawlAndSaveWithKeywords(keywords, 200, 50, "20240101", "20241231");

// 2. 爬取召回记录
usRecall.crawlAndSaveWithKeywords(keywords, 100, 30, "20240101", "20241231");

// 3. 爬取不良事件
usEvent.crawlFDADataWithKeywords(keywords, "01/01/2024", "12/31/2024", 10);

// 4. 爬取欧盟注册
euRegistration.crawlAndSaveWithKeywords(keywords, 200, 50, "", "");
```

### 示例2：爬取特定制造商数据

```java
String manufacturer = "Medtronic";

// 1. 510K申请
us510k.crawlAndSaveByApplicant(manufacturer, 100, 50, "", "");

// 2. 设备注册
usRegistration.crawlAndSaveByManufacturerName(manufacturer, 100, 50, "", "");
```

### 示例3：按HS编码爬取海关数据

```java
List<String> hsCodes = Arrays.asList("9018", "9019", "9021", "9022");

// 美国海关
for (String hsCode : hsCodes) {
    usCustoms.crawlByHsCode(hsCode, 50, 20, LocalDate.of(2024, 1, 1));
}

// 欧盟TARIC
euCustomcase.crawlAndSaveWithTaricCodes(hsCodes, 100, 30);
```

---

## 🎓 高级用法

### 1. 自定义搜索条件（FDA API）

```java
// 组合搜索条件
String searchTerm = "device_name:pacemaker AND device_class:3";
us510k.crawlAndSaveDevice510K(searchTerm, 100, 50, "", "");

// 日期范围搜索
String searchTerm = "openfda.device_name:analyzer";
us510k.crawlAndSaveDevice510K(searchTerm, -1, 50, "20240601", "20241231");
```

### 2. 增量更新

```java
// 只爬取最近30天的新数据
LocalDate thirtyDaysAgo = LocalDate.now().minusDays(30);
String dateFrom = thirtyDaysAgo.format(DateTimeFormatter.ofPattern("yyyyMMdd"));
String dateTo = LocalDate.now().format(DateTimeFormatter.ofPattern("yyyyMMdd"));

us510k.crawlAndSaveDevice510K("device_name:medical", 500, 50, dateFrom, dateTo);
```

### 3. 错误恢复

所有爬虫都会记录爬取状态到 `crawler_checkpoint` 表，包括：
- 爬取类型
- 搜索词
- 进度
- 状态（SUCCESS/FAILED/RUNNING）
- 错误信息

可以通过查询该表了解爬取历史和错误原因。

---

## 📞 技术支持

**相关文档：**
- API文档: `http://localhost:8080/swagger-ui.html`
- 爬虫API文档: `CRAWLER_API_DOCUMENTATION.md`
- 数据库文档: `DATABASE_MIGRATION_GUIDE.md`

**日志查看：**
```bash
# 查看爬虫日志
tail -f spring-boot-backend/logs/application.log
```

**常见问题：**
- 爬取失败：检查网络连接和API Key
- 数据重复：正常现象，爬虫会自动跳过
- 速度慢：调整batchSize和maxRecords参数

